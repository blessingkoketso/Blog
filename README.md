
# Improving the accuracy of a model using a gradient based method.

There are different ways to improve the accuracy of a model. But in this discussion we are going to focus on gradient based methods. Gradient techniques are ensemble methods. But we are particularly interested XGBoost. The algorithm is known for winning competitions for structured data.

What is a XGBoost ?

XGBoost is supervised machine learning algorithm that belongs to the family of boosting algorithms and uses the gradient boosting(GBM) framework. It is an optimized distributed gradient boosting library. The term XGBoost stands for “Extreme Gradient Boosting”. The algorithm works well on distributed systems.  


The XGBoost algorithm is gaining popularity for being better than most machine learning algorithms. It is a “state-of-the-art” machine learning algorithm to deal with structured data. The key to XGBoost high accuracy is the use of boosting. What boosting does is it combines weak learners and strong learners to improve the prediction accuracy. 

Comparing Random Forests and XGBoost :

In this section, we will compare the performance of a Random Forest and XGBoost. We will start with Random Forests. 

We will look at an example to test the performance of Random Forest and XGBoost using a mortgage dataset. The problem we are trying to solve is the prediction of approvals.

The advantage of XGBoost over traditional methods is seen in the execution speed and model performance. It produces superior results due to its software architecture and compatibility to hardware. 

To optimise the XGBoost set the following parameters,  

For max_depth the best value is between 3 and 8 this is the number of trees. The learning_rate should be less than or equal to 0.1 anything learning rate that is small will compromise the prediction.  For n_estimators the best number of estimators to use is between 50 and 150. 

Why use XGBoost

The two reasons to use XGBoost are 
Execution speed.
Model Performance.


It is said that, when you are in doubt use XGBoost.







